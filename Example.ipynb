{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, json, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm # print progress bar\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors, FastText\n",
    "from PretrainedModel import FastSpelling, get_embedding\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Embedding, Dropout, Conv1D, GlobalMaxPooling1D, \\\n",
    "                        Concatenate, Dense, SpatialDropout1D, Bidirectional, GRU\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import activations\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/home/babeau/Documents/Altergrad/Pretrained_models'\n",
    "\n",
    "# .bin file\n",
    "google_300 = {'name': 'google_300', \n",
    "              'init_dimension': 300, \n",
    "              'file': folder_path + '/GoogleNews-vectors-negative300.bin.gz'}\n",
    "\n",
    "# .txt file\n",
    "twitter_200 = {'name': 'twitter_200', \n",
    "               'init_dimension': 200, \n",
    "               'file': folder_path + '/glove.twitter.27B/glove.twitter.27B.200d.txt'}\n",
    "twitter_100 = {'name': 'twitter_100', \n",
    "               'init_dimension': 100, \n",
    "               'file': folder_path + '/glove.twitter.27B/glove.twitter.27B.100d.txt'}\n",
    "twitter_50 = {'name': 'twitter_50', \n",
    "              'init_dimension': 50, \n",
    "              'file': folder_path + '/glove.twitter.27B/glove.twitter.27B.50d.txt'}\n",
    "twitter_25 = {'name': 'twitter_25', \n",
    "              'init_dimension': 25, \n",
    "              'file': folder_path + '/glove.twitter.27B/glove.twitter.27B.25d.txt'}\n",
    "glove_840B_300 = {'name': 'glove_840B_300', \n",
    "                 'init_dimension': 300, \n",
    "                 'file': folder_path + '/glove.840B.300d.txt'}\n",
    "glove_6B_300 = {'name': 'glove_6B_300', \n",
    "               'init_dimension': 300, \n",
    "               'file': folder_path + '/glove.6B/glove.6B.300d.txt'}\n",
    "\n",
    "# .vec file\n",
    "wiki_300 = {'name': 'wiki_300', \n",
    "            'init_dimension': 300, \n",
    "            'file': folder_path + '/wiki-news-300d-1M.vec'}\n",
    "crawl_300 = {'name': 'crawl_300', \n",
    "             'init_dimension': 300, \n",
    "             'file': folder_path + '/crawl-300d-2M.vec'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=1e5)\n",
    "train_text = data['comment_text']\n",
    "\n",
    "tokenizer.fit_on_texts(train_text.values)\n",
    "train_sequences = tokenizer.texts_to_sequences(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [text for text in data['comment_text']]\n",
    "y = data[[col for col in data.columns if col != 'id' and col != 'comment_text' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.json', 'r') as file:\n",
    "    vectorized_tokens = json.load(file)\n",
    "with open('vocab2index.json', 'r') as file:\n",
    "    vocab2index = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pretrained_models = [wiki_300, google_300, twitter_200, glove_840B_300, crawl_300]\n",
    "\n",
    "embeddings_dict = dict()\n",
    "for i in tqdm(range(len(pretrained_models))):\n",
    "    embeddings_dict = dict()\n",
    "    embeddings_dict[pretrained_models[i]['name']] = get_embedding(pretrained_models[i], vocab2index).tolist()\n",
    "    with open('embeddings' + pretrained_models[i]['name'] + '.json', 'w') as file:\n",
    "        json.dump(embeddings_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('embeddings.json', 'r') as file:\n",
    "    dic = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_branches = 2\n",
    "nb_filters = 150\n",
    "filter_sizes = [3,4]\n",
    "drop_rate = 0.2 # amount of dropout regularization\n",
    "batch_size = 512\n",
    "nb_epochs = 6\n",
    "my_optimizer = 'adam'\n",
    "my_patience = 2 # for early stopping strategy\n",
    "\n",
    "max_size = len(vectorized_tokens[1])\n",
    "pretrained_model = wiki_300\n",
    "embeddings = get_embedding(pretrained_model, vocab2index)\n",
    "print('Done embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "embeddings = umap.UMAP(n_components=64, n_neighbors=30, min_dist=0.0, random_state=42).fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# = = = = = defining architecture = = = = =\n",
    "def cnn_branch(n_filters,k_size,d_rate,my_input):\n",
    "    return Dropout(d_rate)(GlobalMaxPooling1D()(Conv1D(filters=n_filters,\n",
    "                                                       kernel_size=k_size,\n",
    "                                                       activation='relu')(my_input)))\n",
    "\n",
    "doc_ints = Input(shape=(None,))\n",
    "doc_wv = Embedding(input_dim= embeddings.shape[0], # vocab size\n",
    "                  output_dim= embeddings.shape[1], # dimension of embedding\n",
    "                  weights = [embeddings],\n",
    "                  input_length=max_size,\n",
    "                  trainable = False,\n",
    "                  )(doc_ints)\n",
    "doc_wv_dr = SpatialDropout1D(drop_rate)(doc_wv)\n",
    "doc_wv_dr = Bidirectional(GRU(nb_filters, return_sequences=True))(doc_wv_dr)\n",
    "branch_outputs = [cnn_branch(nb_filters, filter_sizes[idx], drop_rate, doc_wv_dr) \\\n",
    "                  for idx in range(nb_branches)]\n",
    "concat = Concatenate()(branch_outputs)\n",
    "preds = Dense(units=6, activation='sigmoid')(concat)\n",
    "\n",
    "model = Model(doc_ints, preds)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer = my_optimizer,\n",
    "              metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# = = = = = training = = = = =\n",
    "path_to_data = '/home/babeau/Downloads/all/'\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_acc', # go through epochs as long as accuracy on validation set increases\n",
    "                               patience=my_patience,\n",
    "                               mode='max')\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=path_to_data + 'model_' + pretrained_model['name'], \n",
    "                               verbose=1, \n",
    "                               save_best_only=True)\n",
    "x_train, x_test, y_train, y_test = train_test_split(np.array(vectorized_tokens), np.array(y), \\\n",
    "                                                    test_size = 0.25, random_state = 100)\n",
    "model.fit(x_train,\n",
    "         y_train,\n",
    "         batch_size=batch_size,\n",
    "         epochs=nb_epochs,\n",
    "         validation_data=(x_test, y_test),\n",
    "         callbacks=[early_stopping, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(x_test)\n",
    "mean_score = np.mean([roc_auc_score(y_test[:,i], preds[:,i]) for i in range(preds.shape[1])])\n",
    "mean_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "google 300: 0.9449\n",
    "glove 300: 0.9542"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- clear text\n",
    "- tokenize\n",
    "- check if word is too long (otherwise spelling- correction function will run forever): look at words containing more than xxx characters and decide where is the cut off.\n",
    "- apply spelling- correction over vocabulary\n",
    "- vectorize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ripser import Rips\n",
    "from persim import PersImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 200\n",
    "N_per_class = int(N / 2)\n",
    "N_in_class = 400\n",
    "\n",
    "def noise(N, scale):\n",
    "    return scale * np.random.random((N, 2))\n",
    "\n",
    "def circle(N, scale, offset):\n",
    "    return offset + scale * datasets.make_circles(n_samples=N, factor=0.4, noise=0.05)[0]\n",
    "    \n",
    "just_noise = [noise(N_in_class, 150) for _ in range(N_per_class)]\n",
    "\n",
    "half = int(N_in_class / 2)\n",
    "with_circle = [np.concatenate((circle(half, 50, 70), noise(half, 150)))\n",
    "               for _ in range(N_per_class)]\n",
    "\n",
    "datas = []\n",
    "datas.extend(just_noise)\n",
    "datas.extend(with_circle)\n",
    "\n",
    "# Define labels\n",
    "labels = np.zeros(N)\n",
    "labels[N_per_class:] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = np.random.random((100, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = np.array(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rips = Rips(maxdim=1, coeff=2)\n",
    "diagrams = [rips.fit_transform(data) for data in datas]\n",
    "diagrams_h1 = [rips.fit_transform(data)[1] for data in datas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gudhi as gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd(data,max_dim):\n",
    "    \n",
    "    rips_complex = gd.RipsComplex(data)\n",
    "    Rips_simplex_tree_sample = rips_complex.create_simplex_tree(max_dimension=(max_dim+1)) \n",
    "    Rips_simplex_tree_sample.persistence()\n",
    "    diag_Rips = Rips_simplex_tree_sample.persistence_intervals_in_dimension(max_dim)\n",
    "    return np.array(diag_Rips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rips = Rips(maxdim=1, coeff=2, verbose=False)\n",
    "\n",
    "#%timeit rips_complex = rips.fit_transform(datas[0])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.69 s ± 490 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit diag = pd(datas[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PersImage(pixels=[20, 20], spread=1, specs=None, kernel_type=\"gaussian\", weighting_type=\"linear\")\n"
     ]
    }
   ],
   "source": [
    "pim = PersImage(pixels=[20,20], spread=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15.65368748, 15.91078663],\n",
       "       [15.65347672, 16.11473083],\n",
       "       [15.58119869, 21.68674088],\n",
       "       [15.02328205, 15.3736515 ],\n",
       "       [14.58504009, 17.22622108],\n",
       "       [14.17356873, 14.49266815],\n",
       "       [13.79939175, 13.86249638],\n",
       "       [13.65787315, 20.46810722],\n",
       "       [13.48520184, 17.78808022],\n",
       "       [13.43519592, 17.19465637],\n",
       "       [13.17404366, 14.29042912],\n",
       "       [13.08502197, 14.26892853],\n",
       "       [13.03124142, 17.8809433 ],\n",
       "       [12.99820042, 16.16630745],\n",
       "       [12.95693588, 13.14085865],\n",
       "       [12.89973259, 12.93458271],\n",
       "       [12.74729729, 13.86171818],\n",
       "       [12.52601433, 14.26108265],\n",
       "       [12.43240738, 14.25903225],\n",
       "       [11.92043018, 13.24260712],\n",
       "       [11.9102602 , 13.34253597],\n",
       "       [11.82876015, 13.01293373],\n",
       "       [11.82090473, 15.63753986],\n",
       "       [11.71821022, 17.03198433],\n",
       "       [11.70853615, 15.48952007],\n",
       "       [11.69644451, 14.64228058],\n",
       "       [11.59950733, 15.00278854],\n",
       "       [11.52294922, 12.39854622],\n",
       "       [11.46739388, 13.24874496],\n",
       "       [11.28276539, 16.88393211],\n",
       "       [11.26350594, 13.30395126],\n",
       "       [11.19953537, 17.00808525],\n",
       "       [11.12161636, 12.80666828],\n",
       "       [11.07409191, 13.57712746],\n",
       "       [11.04435158, 13.55633736],\n",
       "       [11.02388096, 16.00860023],\n",
       "       [11.00393581, 14.90107727],\n",
       "       [10.73401928, 11.76649761],\n",
       "       [10.7319746 , 13.70670319],\n",
       "       [10.70811272, 17.25281715],\n",
       "       [10.65878296, 10.87543201],\n",
       "       [10.614398  , 10.9444828 ],\n",
       "       [10.56315517, 18.92446136],\n",
       "       [10.54330349, 11.65358257],\n",
       "       [10.42238808, 13.71125507],\n",
       "       [10.36628151, 11.323843  ],\n",
       "       [10.36608887, 12.17175007],\n",
       "       [10.29271317, 10.96471405],\n",
       "       [10.2683363 , 11.76756382],\n",
       "       [10.20785713, 17.30248451],\n",
       "       [10.05603981, 18.67444611],\n",
       "       [10.05371952, 12.61959171],\n",
       "       [10.05283737, 13.64249134],\n",
       "       [ 9.91722775, 10.30908871],\n",
       "       [ 9.84423733, 10.47531319],\n",
       "       [ 9.82946014, 13.56525612],\n",
       "       [ 9.82339859, 10.74114227],\n",
       "       [ 9.729352  , 10.50455379],\n",
       "       [ 9.65838718, 11.69331169],\n",
       "       [ 9.59106445, 13.26334572],\n",
       "       [ 9.57168198, 10.02918339],\n",
       "       [ 9.551898  , 10.70008469],\n",
       "       [ 9.46118927,  9.78634453],\n",
       "       [ 9.39369011, 17.12952423],\n",
       "       [ 9.34356022, 14.80826664],\n",
       "       [ 9.07963181,  9.40679646],\n",
       "       [ 9.04817104, 11.61085033],\n",
       "       [ 8.9488802 , 14.64899254],\n",
       "       [ 8.89568806,  9.23766041],\n",
       "       [ 8.88929176, 10.37045097],\n",
       "       [ 8.8435688 , 11.97071743],\n",
       "       [ 8.73419094, 12.04534721],\n",
       "       [ 8.66939735,  9.94430828],\n",
       "       [ 8.66236496, 12.27984142],\n",
       "       [ 8.63955593, 11.88136196],\n",
       "       [ 8.60510159,  9.53078175],\n",
       "       [ 8.5228529 , 14.07414055],\n",
       "       [ 8.49583244,  9.64301395],\n",
       "       [ 8.32205009,  9.20578575],\n",
       "       [ 8.07536125,  9.10903645],\n",
       "       [ 7.96297312,  9.41500473],\n",
       "       [ 7.86844826,  9.02336979],\n",
       "       [ 7.82619143,  8.2453022 ],\n",
       "       [ 7.80523443,  7.90956259],\n",
       "       [ 7.77722692, 10.43155003],\n",
       "       [ 7.77568865,  8.52824879],\n",
       "       [ 7.59272957,  9.63921642],\n",
       "       [ 7.24392271,  7.54282427],\n",
       "       [ 6.76072264,  7.64173603],\n",
       "       [ 6.67489338,  7.320292  ],\n",
       "       [ 6.60014534,  7.23648071],\n",
       "       [ 6.47668362,  7.95961428],\n",
       "       [ 6.34498072,  8.99610901],\n",
       "       [ 6.01805115,  6.15403175],\n",
       "       [ 5.66555691,  5.74572277]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rips_complex = rips.fit_transform(datas[0])[1]\n",
    "rips_complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.random((1000, 30))\n",
    "y = np.random.random((100, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.34 s ± 425 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dat = rips.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.1 ms ± 1.06 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dat = rips.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag = pd(datas[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.6655569 ,  5.74572271],\n",
       "       [ 6.01805092,  6.15403189],\n",
       "       [ 6.60014513,  7.23648071],\n",
       "       [ 6.67489344,  7.32029215],\n",
       "       [ 7.24392265,  7.54282417],\n",
       "       [ 6.7607228 ,  7.64173608],\n",
       "       [ 7.80523457,  7.90956272],\n",
       "       [ 6.47668362,  7.95961408],\n",
       "       [ 7.82619119,  8.24530201],\n",
       "       [ 7.7756888 ,  8.5282489 ],\n",
       "       [ 6.34498052,  8.99610867],\n",
       "       [ 7.86844807,  9.02337022],\n",
       "       [ 8.07536171,  9.10903666],\n",
       "       [ 8.3220505 ,  9.2057855 ],\n",
       "       [ 8.8956877 ,  9.23766073],\n",
       "       [ 9.07963183,  9.40679611],\n",
       "       [ 7.9629731 ,  9.41500466],\n",
       "       [ 8.60510196,  9.53078197],\n",
       "       [ 7.59272934,  9.63921669],\n",
       "       [ 8.49583276,  9.64301434],\n",
       "       [ 9.46118947,  9.78634409],\n",
       "       [ 8.66939703,  9.94430868],\n",
       "       [ 9.5716821 , 10.0291838 ],\n",
       "       [ 9.91722801, 10.30908878],\n",
       "       [ 8.88929172, 10.37045094],\n",
       "       [ 7.77722712, 10.43154981],\n",
       "       [ 9.84423702, 10.47531291],\n",
       "       [ 9.72935217, 10.50455347],\n",
       "       [ 9.5518981 , 10.70008509],\n",
       "       [ 9.8233987 , 10.74114267],\n",
       "       [10.65878291, 10.87543172],\n",
       "       [10.6143978 , 10.94448302],\n",
       "       [10.29271297, 10.9647145 ],\n",
       "       [10.36628147, 11.3238426 ],\n",
       "       [ 9.04817058, 11.61084997],\n",
       "       [10.54330306, 11.65358224],\n",
       "       [ 9.65838675, 11.69331168],\n",
       "       [10.73401918, 11.76649799],\n",
       "       [10.26833664, 11.76756398],\n",
       "       [ 8.6395563 , 11.88136238],\n",
       "       [ 8.84356865, 11.97071728],\n",
       "       [ 8.73419106, 12.04534752],\n",
       "       [10.3660891 , 12.17175003],\n",
       "       [ 8.66236456, 12.2798419 ],\n",
       "       [11.52294946, 12.39854653],\n",
       "       [10.05371937, 12.61959138],\n",
       "       [11.12161643, 12.80666858],\n",
       "       [12.89973237, 12.93458282],\n",
       "       [11.82875997, 13.01293336],\n",
       "       [12.95693606, 13.14085832],\n",
       "       [11.92042987, 13.24260685],\n",
       "       [11.46739385, 13.24874472],\n",
       "       [ 9.5910641 , 13.26334619],\n",
       "       [11.26350562, 13.30395144],\n",
       "       [11.91026023, 13.34253578],\n",
       "       [11.044352  , 13.55633717],\n",
       "       [ 9.82946044, 13.56525659],\n",
       "       [11.0740915 , 13.57712789],\n",
       "       [10.05283723, 13.64249089],\n",
       "       [10.73197425, 13.70670358],\n",
       "       [10.42238773, 13.71125536],\n",
       "       [12.7472976 , 13.86171822],\n",
       "       [13.79939147, 13.86249656],\n",
       "       [ 8.52285256, 14.07414098],\n",
       "       [12.43240743, 14.2590318 ],\n",
       "       [12.52601452, 14.26108254],\n",
       "       [13.08502227, 14.26892811],\n",
       "       [13.17404389, 14.29042873],\n",
       "       [14.17356862, 14.49266841],\n",
       "       [11.69644484, 14.64228046],\n",
       "       [ 8.94888059, 14.64899293],\n",
       "       [ 9.3435599 , 14.80826707],\n",
       "       [11.00393596, 14.90107739],\n",
       "       [11.59950766, 15.0027886 ],\n",
       "       [15.02328239, 15.37365179],\n",
       "       [11.7085362 , 15.48952029],\n",
       "       [11.82090464, 15.63753991],\n",
       "       [15.65368705, 15.91078681],\n",
       "       [11.02388119, 16.00859944],\n",
       "       [15.65347674, 16.11473095],\n",
       "       [12.99820039, 16.16630688],\n",
       "       [11.28276492, 16.88393213],\n",
       "       [11.19953552, 17.00808497],\n",
       "       [11.71821027, 17.03198418],\n",
       "       [ 9.39369002, 17.12952482],\n",
       "       [13.43519551, 17.19465545],\n",
       "       [14.58504046, 17.22622193],\n",
       "       [10.70811238, 17.25281684],\n",
       "       [10.20785759, 17.30248434],\n",
       "       [13.4852019 , 17.78808106],\n",
       "       [13.03124162, 17.88094423],\n",
       "       [10.05604025, 18.67444681],\n",
       "       [10.56315562, 18.92446159],\n",
       "       [13.65787354, 20.46810789],\n",
       "       [15.58119892, 21.68674112]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diag"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
